---
layout: default
---
<!DOCTYPE html>
<html>
  <body>
    <h1>Implementations</h1>
	  <ul>
		  <li>
		<p>I have implemented a renderer by following the book PBRT. It can do rasterization, 
		ray tracing and path tracing. I am planning to add bidirectional path tracing and 
		photon mapping in the near future. </p>
		<p>The path tracing is composed of direct and indirect lighting. Importance sampling is also 
		used during the sampling of the light source and direction. A cornell box example can be seen below.</p>
		<img src=" https://dl.dropboxusercontent.com/s/h8dibntwxp0g4ze/Rendering1.JPG?dl=0" style = "width:200px;height:200px" >
		<img src="https://dl.dropboxusercontent.com/s/k8lg9cmop2f85ol/Rendering2.JPG?dl=0" style = "width:200px;height:200px">
		<img src="https://dl.dropboxusercontent.com/s/q20k5iol3i7s4db/Rendering3.JPG?dl=0" style = "width:200px;height:200px">
		<p>Missing parts are the bounding box volume hiearchy and antialiasing. I will add them soon. The code 
		for the implementation can be found on my github account: ... </p>
		  </li>
		  <li>
		  <p>I have implemented a vanilla and wasserstein GAN to learn illumination from low resolution fish-eye images. 
			Vanilla GANs showed low accuracy in learning the high dynamic range of sky lighting. Wasserstein GANs 
			performed better on upsampling the HDR low resolution fish-eye images.</p>
		<img src=" https://dl.dropboxusercontent.com/s/usmruf2bs3yvhtk/GAN_Original.JPG?dl=0" style = "width:300px;height:300px" align="middle" >
		<img src=" https://dl.dropboxusercontent.com/s/jn2zixz734ey1ci/GAN_Output.JPG?dl=0" style = "width:300px;height:300px" align="middle">
		  <p>Fisheye image on the left shows the input high resolution cloudy sky illumination (tonemapped HDR image). The one
		  on the right shows generated by wasserstein GAN. Training is implemented on the patches extracted from the 
		  fisheye images. The method fails to learn the edges of the clouds where there is
		  much contrast between the cloud and the sky. </p>
		  </li>
	  </u1>
    <h1>Publications</h1>
    <ul>
      <li>P. Satilmis, T. Bashford-Rogers, K.Debattista, and A. Chalmers. “A Machine
Learning Driven Sky Model” (IEEE, Computer Graphics and Application, 2016.)</li>
      <video width="640" height="480" autoplay>
      <source src="https://dl.dropboxusercontent.com/s/gph48lt5mp5es0c/MLSkyModel.mp4" type="video/mp4">
      Your browser does not support the video tag.
      </video>
    </ul>  

    <p>Sky lighting is an important part of realistic rendering. But, memory consumption and evaluation speed can be a big concern 
	    especially in real time application. Artificial Neural Networks can learn a continuous function with a single hidden 
	    layer (the universal approximation theorem).
	    This makes them an efficient method to represent the lighting from Environment Maps (HDR images) and also analytical 
	    functions. Based on the universal approximation theorem, I have represented the skylighting with an eight dimensional 
	    feature vector.
	    In this work, I did research on backpropagation algorithm. I have deep understanding of feedforward networks,
	    and impact of cost functions on the results. A juxtaposition example generated by the method can be seen in the 
	    following image:</p>
	    <img src=" https://dl.dropboxusercontent.com/s/rj712pvap5nv87o/Juxtaposition.png?dl=0" style = "width:600px;height:300px" >
		

  </body>
</html>
